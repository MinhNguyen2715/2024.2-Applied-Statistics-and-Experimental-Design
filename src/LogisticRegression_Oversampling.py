# -*- coding: utf-8 -*-
"""AS&ED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ikAKIHEwCPmowO9z3SJUQb85IXaXFMIi

# Import libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import (
    train_test_split, GridSearchCV, RandomizedSearchCV,
    StratifiedKFold, cross_validate, cross_val_score
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    make_scorer, average_precision_score, classification_report,
    confusion_matrix, ConfusionMatrixDisplay
)
from sklearn.datasets import make_classification
from scipy.stats import uniform, loguniform

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

from google.colab import drive
drive.mount('/mnt/drive')

"""# Import data"""

# data = pd.read_excel('/mnt/drive/MyDrive/HUST/AS&ED/creditcard.xlsx')
data = pd.read_csv('/mnt/drive/MyDrive/HUST/AS&ED/creditcard.csv')

data.head(5)

"""# Logistic regresion + lí thuyết về credit card fraud system
0.172% of all transactions.

# Method 1: Using undersampling

## Step 1: Split Data (Stratified)

## Step 2: Apply Resampling (Undersampling)

## Step 3: Feature Scaling (StandardScaler)

## Step 4: Model Training with Class Weights

## Step 5: Hyperparameter Tuning

## Step 6: Evaluate on Test Set

# Method 2: Using oversampling

## Step 1: Split Data (Stratified)
"""

X = data.drop('Class', axis=1)
y = data['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

print(Counter(y_test))

"""## Step 2: Apply Resampling (Oversampling)

"""

smote = SMOTE(random_state=42, sampling_strategy=0.2)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(Counter(y_train_resampled))
print(X_train_resampled.shape)
print(y_train_resampled.shape)

"""## Step 3: Feature Scaling (StandardScaler)"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

print(X_train_scaled.shape)

"""## Step 4: Hyperparameter Tuning"""

param_grid = [
    {
        'penalty': ['l1'],
        'solver': ['liblinear', 'saga'],
        'C': [0.0005, 0.001, 0.002],
        'class_weight': [None, 'balanced']
    },
    {
        'penalty': ['l2'],
        'solver': ['liblinear', 'lbfgs', 'saga'],
        'C': [0.0005, 0.001, 0.002],
        'class_weight': [None, 'balanced']
    },
    {
        'penalty': ['elasticnet'],
        'solver': ['saga'],
        'C': [0.0005, 0.001, 0.002],
        'class_weight': [None, 'balanced'],
        'l1_ratio': [0.1, 0.5, 0.9]
    }
]


model = LogisticRegression(max_iter=1000, random_state=42)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scoring = 'average_precision'

grid_search = GridSearchCV(
    model,
    param_grid=param_grid,
    scoring='average_precision',
    cv=cv,
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train_scaled, y_train_resampled)

best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)
print("Best AUPRC score:", grid_search.best_score_)

tmp_model = LogisticRegression()
# param_dist = {
#     'penalty': ['l1', 'l2', 'elasticnet'],
#     'solver': ['liblinear', 'lbfgs', 'saga'],
#     'C': loguniform(1e-4, 10),
#     'class_weight': [None, 'balanced'],
#     'l1_ratio': [0.1, 0.5, 0.9]  # only used when penalty='elasticnet'
# }

# random_search = RandomizedSearchCV(
#     estimator=model,
#     param_distributions=param_dist,
#     n_iter=30,               # Number of combinations to try
#     scoring=scoring,
#     cv=5,
#     random_state=42,
#     n_jobs=-1,
#     verbose=1
# )

# # Fit on your scaled/resampled training data
# random_search.fit(X_train_scaled, y_train_resampled)

# # Best results
# print("Best Parameters:", random_search.best_params_)
# print("Best AUPRC:", random_search.best_score_)

"""## Step 5: Evaluate on Test Set"""

from sklearn.metrics import (
    classification_report, precision_score, recall_score, f1_score,
    average_precision_score, roc_auc_score, roc_curve,
    precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay
)
import matplotlib.pyplot as plt
import numpy as np

tmp_model = LogisticRegression(
    C=0.002,
    class_weight=None,
    penalty='l2',
    solver='liblinear'
)

# best_model = grid_search.best_estimator_
tmp_model.fit(X_train_scaled, y_train_resampled)

# Predict probabilities and class labels
y_proba_test = tmp_model.predict_proba(X_test_scaled)[:, 1]
y_pred_test = tmp_model.predict(X_test_scaled)

# Calculate core metrics
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)
auprc = average_precision_score(y_test, y_proba_test)
roc_auc = roc_auc_score(y_test, y_proba_test)

# Print classification report
print("Classification Report:\n", classification_report(y_test, y_pred_test))
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print(f"AUPRC: {auprc:.4f}")

# Precision@K function
def precision_at_k(y_true, y_scores, k):
    k = min(k, len(y_scores))
    indices = np.argsort(y_scores)[::-1]
    top_k = y_true[indices[:k]]
    return np.sum(top_k) / k

p_at_100 = precision_at_k(np.array(y_test), y_proba_test, k=100)
print(f"Precision@100: {p_at_100:.4f}")

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_test)
RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
plt.title("ROC Curve (AUC = {:.4f})".format(roc_auc))
plt.grid(True)
plt.show()

# Plot Precision-Recall Curve
precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba_test)
PrecisionRecallDisplay(precision=precision_vals, recall=recall_vals).plot()
plt.title("Precision-Recall Curve (AUPRC = {:.4f})".format(auprc))
plt.grid(True)
plt.show()